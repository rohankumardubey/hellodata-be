{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to HelloDATA BE \ud83d\udc4b\ud83c\udffb","text":"<p>This is the open documentation about HelloDATA BE. We hope you enjoy it.</p> <p>Contribute</p> <p>In case something is missing or you'd like to add something, below is how you can contribute:</p> <ul> <li>Star our\u00a0GitHub \u2b50</li> <li>Want to discuss, contribute, or need help, create a GitHub Issue, create a Pull Request or open a Discussion.</li> </ul>"},{"location":"#what-is-hellodata-be","title":"What is HelloDATA BE?","text":"<p>HelloDATA BE is an\u00a0enterprise data platform\u00a0built on top of open-source tools based on the modern data stack. We use state-of-the-art tools such as dbt for data modeling with SQL and Airflow to run and orchestrate tasks and use Superset to visualize the BI dashboards. The underlying database is Postgres.</p> <p>Each of these components is carefully chosen and additional tools can be added in a later stage.</p>"},{"location":"#why-do-you-need-an-open-enterprise-data-platform-hellodata-be","title":"Why do you need an Open Enterprise Data Platform (HelloDATA BE)?","text":"<p>These days the amount of data grows yearly more than the entire lifetime before. Each fridge, light bulb, or anything really starts to produce data. Meaning there is a growing need to make sense of more data. Usually, not all data is necessary and valid, but due to the nature of growing data, we must be able to collect and store them easily. There is a great need to be able to analyze this data. The result can be used for secondary usage and thus create added value.</p> <p>That is what this open data enterprise platform is all about. In the old days, you used to have one single solution provided; think of SAP or Oracle. These days that has completely changed. New SaaS products are created daily, specializing in a tiny little niche. There are also many open-source tools to use and get going with minutes freely.</p> <p>So why would you need a HelloDATA BE? It's simple. You want the best of both worlds. You want\u00a0open source\u00a0to not be locked-in, to use the strongest, collaboratively created product in the open. People worldwide can fix a security bug in minutes, or you can even go into the source code (as it's available for everyone) and fix it yourself\u2014compared to an extensive vendor where you solely rely on their update cycle.</p> <p>But let's be honest for a second if we use the latest shiny thing from open source. There are a lot of bugs, missing features, and independent tools. That's precisely where HelloDATA BE comes into play. We are building the\u00a0missing platform\u00a0that combines the best-of-breed open-source technologies into a\u00a0single portal, making it enterprise-ready by adding features you typically won't get in an open-source product. Or we fix bugs that were encountered during our extensive tests.</p> <p>Sounds too good to be true? Give it a try. Do you want to knot the best thing? It's open-source as well. Check out our\u00a0GitHub HelloDATA BE.</p>"},{"location":"#quick-start-for-developers","title":"Quick Start for Developers","text":"<p>Want to run HelloDATA BE and test it locally? Run the following command in the docker-compose directory to deploy all components:</p> <pre><code>cd hello-data-deployment/docker-compose\ndocker-compose up -d\n</code></pre> <p>Note: Please refer to our docker-compose README for more information; there are some must presets you need to configure.</p>"},{"location":"architecture/architecture/","title":"Architecture","text":""},{"location":"architecture/architecture/#components","title":"Components","text":"<p>This chapter will explain the core architectural component, its context views, and how HelloDATA works under the hood.</p>"},{"location":"architecture/architecture/#domain-view","title":"Domain View","text":"<p>We separate between two main domains, \"Business and Data Domain.</p>"},{"location":"architecture/architecture/#business-vs-data-domain","title":"Business\u00a0vs. Data Domain","text":"<ul> <li>\"Business\" domain: This domain holds one customer or company with general services (portal, orchestration, docs, monitoring, logging). Every business domain represents a business tenant. HelloDATA is running on a Kubernetes cluster. A business domain is treated as a dedicated namespace within that cluster; thus, multi-tenancy is set up by various namespaces in the Kubernetes cluster.</li> <li>Data Domain: This is where actual data is stored (db-schema). We combine it with a superset instance (related dashboards) and the documentation about these data. Currently, a business domain relates 1 - n to its Data Domains. Within an existing Business Domain a Data Domain can be spawned using Kubernetes deployment features and scripts to set up the database objects.</li> </ul> <p>Resources encapsulated inside a\u00a0Data Domain\u00a0can be:</p> <ul> <li>Schema of the Data Domain</li> <li>Data mart tables of the Data Domain</li> <li>The entire DWH environment of the Data Domain</li> <li>Data lineage documents of the DBT projects of the Data Domain.     Dashboards, charts, and datasets within the superset instance of a Data Domain.</li> <li>Airflow DAGs of the Data Domain.</li> </ul> <p>On top, you can add\u00a0subsystems. This can be seen as extensions that make HelloDATA pluggable with additional tools. We now support\u00a0CloudBeaver\u00a0for viewing your Postgres databases, RtD, and Gitea. You can imagine adding almost infinite tools with capabilities you'd like to have (data catalog, semantic layer, specific BI tool, Jupyter Notebooks, etc.).</p> <p>Read more about Business and Data Domain access rights in\u00a0Roles / Authorization Concept.</p> <p></p>"},{"location":"architecture/architecture/#data-domain","title":"Data Domain","text":"<p>Zooming into several Data Domains that can exist within a Business domain, we see an example of Data Domain A-C. Each Data Domain has a persistent storage, in our case, Postgres (see more details in the\u00a0Infrastructure Storage\u00a0chapter below).</p> <p>Each data domain might import different source systems; some might even be used in several data domains, as illustrated. Each Data Domain is meant to have its data model with straightforward to, in the best case, layered data models as shown on the image with:</p>"},{"location":"architecture/architecture/#landingstaging-area","title":"Landing/Staging Area","text":"<p>Data from various source systems is first loaded into the Landing/Staging Area.</p> <ul> <li>In this first area, the data is stored as it is delivered; therefore, the stage tables' structure corresponds to the interface to the source system.</li> <li>No relationships exist between the individual tables.</li> <li>Each table contains the data from the final delivery, which will be deleted before the next delivery.</li> <li>For example, in a grocery store, the Staging Area corresponds to the loading dock where suppliers (source systems) deliver their goods (data). Only the latest deliveries are stored there before being transferred to the next area.</li> </ul>"},{"location":"architecture/architecture/#data-storage-cleansing-area","title":"Data Storage (Cleansing Area)","text":"<p>It must be cleaned before the delivered data is loaded into the Data Processing (Core). Most of these cleaning steps are performed in this area.</p> <ul> <li>Faulty data must be filtered, corrected, or complemented with singleton (default) values.</li> <li>Data from different source systems must be transformed and integrated into a unified form.</li> <li>This layer also contains only the data from the final delivery.</li> <li>For example, In a grocery store, the Cleansing Area can be compared to the area where the goods are commissioned for sale. The goods are unpacked, vegetables and salad are washed, the meat is portioned, possibly combined with multiple products, and everything is labeled with price tags. The quality control of the delivered goods also belongs in this area.</li> </ul>"},{"location":"architecture/architecture/#data-processing-core","title":"Data Processing\u00a0(Core)","text":"<p>The data from the different source systems are brought together in a central area, the Data Processing (Core), through the Landing and Data Storage and stored there for extended periods, often several years.\u00a0</p> <ul> <li>A primary task of this layer is to integrate the data from different sources and store it in a thematically structured way rather than separated by origin.</li> <li>Often, thematic sub-areas in the Core are called \"Subject Areas.\"</li> <li>The data is stored in the Core so that historical data can be determined at any later point in time.\u00a0</li> <li>The Core should be the only data source for the Data Marts.</li> <li>Direct access to the Core by users should be avoided as much as possible.</li> </ul>"},{"location":"architecture/architecture/#data-mart","title":"Data Mart","text":"<p>Subsets of the data from the Core are stored in a form suitable for user queries.\u00a0</p> <ul> <li>Each Data Mart should only contain the data relevant to each application or a unique view of the data. This means several Data Marts are typically defined for different user groups and BI applications.</li> <li>This reduces the complexity of the queries, increasing the acceptance of the DWH system among users.</li> <li>For example, The Data Marts are the grocery store's market stalls or sales points. Each market stand offers a specific selection of goods, such as vegetables, meat, or cheese. The goods are presented so that they are accepted, i.e., purchased, by the respective customer group.</li> </ul> <p>Between the layers, we have lots of\u00a0Metadata</p> <p>Different types of metadata are needed for the smooth operation of the Data Warehouse. Business metadata contains business descriptions of all attributes, drill paths, and aggregation rules for the front-end applications and code designations. Technical metadata describes, for example, data structures, mapping rules, and parameters for ETL control. Operational metadata contains all log tables, error messages, logging of ETL processes, and much more. The metadata forms the infrastructure of a DWH system and is described as \"data about data\".</p> <p></p>"},{"location":"architecture/architecture/#example-multiple-superset-dashboards-within-a-data-domain","title":"Example: Multiple Superset Dashboards within a Data Domain","text":"<p>Within a Data Domain, several users build up different dashboards. Think of a dashboard as a specific use case e.g., Covid, Sales, etc., that solves a particular purpose. Each of these dashboards consists of individual charts and data sources in superset. Ultimately, what you see in the HelloDATA portal are the dashboards that combine all of the sub-components of what Superset provides.</p> <p></p>"},{"location":"architecture/architecture/#portalui-view","title":"Portal/UI View","text":"<p>As described in the intro. The portal is the heart of the HelloDATA application, with access to all critical applications.</p> <p>Entry page of helloDATA: When you enter the portal for the first time, you land on the dashboard where you have</p> <ol> <li>Navigation to jump to the different capabilities of helloDATA</li> <li>Extended status information about<ol> <li>data pipelines, containers, performance, and security</li> <li>documentation and subscriptions</li> </ol> </li> <li>User and profile information of logged-in users.\u00a0</li> <li>Choosing the data domain you want to work within your business domain</li> <li>Overview of your dashboards</li> <li>dbt linage docs</li> <li>Data marts of your Postgres database</li> <li>Answers to freuqently asked questions</li> </ol> <p>More technical details are in the \"Module deployment view\" chapter below.</p> <p></p>"},{"location":"architecture/architecture/#module-view-and-communication","title":"Module View and Communication","text":""},{"location":"architecture/architecture/#modules","title":"Modules","text":"<p>Going one level deeper, we see that we use different modules to make the portal and helloDATA work.\u00a0</p> <p>We have the following modules:</p> <ul> <li>Keycloak: Open-source identity and access management. This handles everything related to user permissions and roles in a central place that we integrate into helloDATA.</li> <li>Redis: open-source, in-memory data store that we use for persisting technical values for the portal to work.\u00a0</li> <li>NATS: Open-source connective technology for the cloud. It handles communication with the different tools we use.</li> <li>Data Stack: We use the open-source data stack with dbt, Airflow, and Superset. See more information in the intro chapters above. Subsystems can be added on demand as extensible plugins.</li> </ul> <p></p>"},{"location":"architecture/architecture/#what-is-keycloak-and-how-does-it-work","title":"What is Keycloak and how does it work?","text":"<p>At the center are two components, NATS and Keycloak.\u00a0Keycloak, together with the HelloDATA portal, handles the authentication, authorization, and permission management of HelloDATA components. Keycloak is a powerful open-source identity and access management system. Its primary benefits include:</p> <ol> <li>Ease of Use: Keycloak is easy to set up and use and can be deployed on-premise or in the cloud.</li> <li>Integration: It integrates seamlessly with existing applications and systems, providing a secure way of authenticating users and allowing them to access various resources and services with a single set of credentials.</li> <li>Single Sign-On: Keycloak takes care of user authentication, freeing applications from having to handle login forms, user authentication, and user storage. Users can log in once and access all applications linked to Keycloak without needing to re-authenticate. This extends to logging out, with Keycloak offering single sign-out across all linked applications.</li> <li>Identity Brokering and Social Login: Keycloak can authenticate users with existing OpenID Connect or SAML 2.0 Identity Providers and easily enable social network logins without requiring changes to your application's code.</li> <li>User Federation: Keycloak has the capacity to connect to existing LDAP or Active Directory servers and can support custom providers for users stored in other databases.</li> <li>Admin Console: Through the admin console, administrators can manage all aspects of the Keycloak server, including features, identity brokering, user federation, applications, services, authorization policies, user permissions, and sessions.</li> <li>Account Management Console: Users can manage their own accounts, update profiles, change passwords, setup two-factor authentication, manage sessions, view account history, and link accounts with additional identity providers if social login or identity brokering has been enabled.</li> <li>Standard Protocols: Keycloak is built on standard protocols, offering support for OpenID Connect, OAuth 2.0, and SAML.</li> <li>Fine-Grained Authorization Services: Beyond role-based authorization, Keycloak provides fine-grained authorization services, enabling the management of permissions for all services from the Keycloak admin console. This allows for the creation of specific policies to meet unique needs. Within HelloDATA, the HelloDATA portal manages authorization, yet if required by upcoming subsystems, this KeyCloak feature can be utilized in tandem.</li> <li>Two-Factor Authentication (2FA): This optional feature of KeyCloak enhances security by requiring users to provide two forms of authentication before gaining access, adding an extra layer of protection to the authentication process.</li> </ol>"},{"location":"architecture/architecture/#what-is-nats-and-how-does-it-work","title":"What is NATS and how does it work?","text":"<p>On the other hand, NATS is central for handling communication between the different modules. Its power comes from integrating modern distributed systems. It is the glue between microservices, making and processing statements, or stream processing.</p> <p>NATS focuses on hyper-connected moving parts and additional data each module generates. It supports location independence and mobility, whether the backend process is streaming or otherwise, and securely handles all of it.</p> <p>NATs let you connect mobile frontend or microservice to connect flexibly. There is no need for static 1:1 communication with a hostname, IP, or port. On the other hand, NATS lets you m:n connectivity based on subject instead. Still, you can use 1:1, but on top, you have things like load balancers, logs, system and network security models, proxies, and, most essential for us,\u00a0sidecars. We use sidecars heavily in connection with NATS.</p> <p>NATS can be\u00a0deployed\u00a0nearly anywhere: on bare metal, in a VM, as a container, inside K8S, on a device, or in whichever environment you choose. And all fully secure.</p>"},{"location":"architecture/architecture/#subsystem-communication","title":"Subsystem communication","text":"<p>Here is an example of subsystem communication. NATS, obviously at the center, handles these communications between the HelloDATA platform and the subsystems with its workers, as seen in the image below.</p> <p>The HelloDATA portal has workers.\u00a0These workers are deployed as extra containers with sidecars, called \"Sidecar Containers\". Each module needing communicating needs a sidecar with these workers deployed to communicate with NATS. Therefore, the subsystem itself has its workers to share with NATS as well.</p> <p></p>"},{"location":"architecture/architecture/#messaging-component-workers","title":"Messaging component workers","text":"<p>Everything starts with a\u00a0web browser\u00a0session. The HelloDATA user accesses the\u00a0HelloDATA Portal\u00a0through HTTP. Before you see any of your modules or components, you must authorize yourself again, Keycloak. Once logged in, you have a Single Sign-on Token that will give access to different business domains or data domains depending on your role.</p> <p>The HelloDATA portal sends an event to the EventWorkers via JDBC to the Portal database.\u00a0The\u00a0portal database\u00a0persists settings from the portal and necessary configurations.</p> <p>The\u00a0EventWorkers, on the other side communicate with the different\u00a0HelloDATA Modules\u00a0discussed above (Keycloak, NATS, Data Stack with dbt, Airflow, and Superset) where needed. Each module is part of the domain view, which persists their data within their datastore.</p> <p></p>"},{"location":"architecture/architecture/#flow-chart","title":"Flow Chart","text":"<p>In this flow chart, you see again what we discussed above in a different way. Here, we\u00a0assign a new user role. Again, everything starts with the HelloDATA Portal and an existing session from Keycloak. With that, the portal worker will publish a JSON message via UserRoleEvent to NATS. As\u00a0the\u00a0communication hub for HelloDATA, NATS knows what to do with each message and sends it to the respective subsystem worker.</p> <p>Subsystem workers will execute that instruction and create and populate roles on, e.g., Superset and Airflow, and once done, inform the spawned subsystem worker that it's done. The worker will push it back to NATS, telling the portal worker, and at the end, will populate a message on the HelloDATA portal.</p> <p></p>"},{"location":"architecture/architecture/#building-block-view","title":"Building Block View","text":""},{"location":"architecture/data-stack/","title":"Data Stack","text":"<p>We'll explain which data stack is behind HelloDATA BE.</p>"},{"location":"architecture/data-stack/#control-pane-portal","title":"Control Pane - Portal","text":"<p>The\u00a0differentiator of HelloDATA\u00a0lies in the Portal. It combines all the loosely open-source tools into a single control pane.</p> <p>The portal lets you see:</p> <ul> <li>Data models with a dbt lineage: You see the sources of a given table or even column.</li> <li>You can check out the latest runs. Gives you when the dashboards have been updated.</li> <li>Create and view all company-wide reports and dashboards.</li> <li>View your data tables as Data Marts: Accessing physical tables, columns, and schemas.</li> <li>Central Monitoring of all processes running in the portal.</li> <li>Manage and control all your user access and role permission and authorization.</li> </ul> <p>You can find more about the navigation and the features in the\u00a0User Manual.</p>"},{"location":"architecture/data-stack/#data-modeling-with-sql-dbt","title":"Data Modeling with SQL - dbt","text":"<p>dbt\u00a0is a small database toolset that has gained immense popularity and is the facto standard for working with SQL. Why, you might ask? SQL is the most used language besides Python for data engineers, as it is\u00a0declarative and easy to learn the basics, and many business analysts or people working with Excel or similar tools might know a little already.</p> <p>The declarative approach is handy as you only define the\u00a0what, meaning you determine what columns you want in the SELECT and which table to query in the FROM statement. You can do more advanced things with WHERE, GROUP BY, etc., but you do not need to care about the\u00a0how. You do not need to watch which database, which partition it is stored, what segment, or what storage. You do not need to know if an index makes sense to use. All of it is handled by the\u00a0query optimizer\u00a0of Postgres (or any database supporting SQL).</p> <p>But let's face it: SQL also has its downside. If you have worked extensively with SQL, you know the spaghetti code that usually happens when using it. It's an issue because of the repeatability\u2014no\u00a0variable\u00a0we can set and reuse in an SQL. If you are familiar with them, you can achieve a better structure with\u00a0CTEs, which allows you to define specific queries as a block to reuse later. But this is only within one single query and handy if the query is already log.</p> <p>But what if you'd like to define your facts and dimensions as a separate query and reuse that in another query? You'd need to decouple the queries from storage, and we would persist it to disk and use that table on disk as a FROM statement for our following query. But what if we change something on the query or even change the name we won't notice in the dependent queries? And we will need to find out which queries depend on each other. There is no\u00a0lineage\u00a0or dependency graph.</p> <p>It takes a lot of work to be organized with SQL. There is also not a lot of support if you use a database, as they are declarative. You need to make sure how to store them in git or how to run them.</p> <p>That's where dbt comes into play. dbt lets you\u00a0create these dependencies within SQL. You can declaratively build on each query, and you'll get errors if one changes but not the dependent one. You get a lineage graph (see an\u00a0example), unit tests, and more. It's like you have an assistant that helps you do your job. It's added software engineering practice that we stitch on top of SQL engineering.</p> <p>The danger we need to be aware of, as it will be so easy to build your models, is not to make 1000 of 1000 tables. As you will get lots of errors checked by the pre-compiling dbt, \u00a0good data modeling techniques are essential to succeed.</p> <p>Below, you see dbt docs, lineage, and templates: 1. Project Navigation 2. Detail Navigation 3. SQL Template 4. SQL Compiled (practical SQL that gets executed) 5. Full Data lineage where with the source and transformation for the current object</p> <p></p> <p>Or zoom dbt lineage (when clicked): </p>"},{"location":"architecture/data-stack/#task-orchestration-airflow","title":"Task Orchestration - Airflow","text":"<p>Airflow\u00a0is the natural next step. If you have many SQLs representing your business metrics, you want them to run on a daily or hourly schedule triggered by events. That's where Airflow comes into play. Airflow is, in its simplest terms, a task or workflow scheduler, which tasks or\u00a0DAGs\u00a0(how they are called) can be written programatically with Python. If you know\u00a0cron\u00a0jobs, these are the lowest task scheduler in Linux (think <code>* * * * *</code>), but little to no customization beyond simple time scheduling.</p> <p>Airflow is different. Writing the DAGs in Python allows you to do whatever your business logic requires before or after a particular task is started. In the past, ETL tools like Microsoft SQL Server Integration Services (SSIS) and others were widely used. They were where your data transformation, cleaning and normalisation took place. In more modern architectures, these tools aren\u2019t enough anymore. Moreover, code and data transformation logic are much more valuable to other data-savvy people (data anlysts, data scientists, business analysts) in the company instead of locking them away in a propreitary format.</p> <p>Airflow or a general Orchestrator ensures correct execution of depend tasks. It is very flexibile and extensible with operators from the community or in-build capabiliities of the framework itself.</p>"},{"location":"architecture/data-stack/#default-view","title":"Default View","text":"<p>Airflow DAGs - Entry page which shows you the status of all your DAGs - what's the schedule of each job - are they active, how often have they failed, etc.</p> <p>Next, you can click on each of the DAGs and get into a detailed view: </p>"},{"location":"architecture/data-stack/#airflow-operations-overview-for-one-dag","title":"Airflow operations overview for one DAG","text":"<ol> <li>General visualization possibilities which you prefer to see (here Grid view)</li> <li>filter your DAG runs</li> <li>see details on each run status in one view\u00a0</li> <li>Check details in the table view</li> <li>Gantt view for another example to see how long each sub-task had of the DAG</li> </ol>"},{"location":"architecture/data-stack/#graph-view-of-dag","title":"Graph view of DAG","text":"<p>It shows you the dependencies of your business's various tasks, ensuring that the order is handled correctly.</p> <p></p>"},{"location":"architecture/data-stack/#dashboards-superset","title":"Dashboards - Superset","text":"<p>Superset\u00a0is the entry point to your data. It's a popular open-source business intelligence dashboard tool that visualizes your data according to your needs.\u00a0It's able to handle all the latest chart types. You can combine them into dashboards filtered and drilled down as expected from a BI tool. The access to dashboards is restricted to authenticated users only. A user can be given view or edit rights to individual dashboards using roles and permissions. Public access to dashboards is not supported.</p>"},{"location":"architecture/data-stack/#example-dashboard","title":"Example dashboard","text":""},{"location":"architecture/data-stack/#supported-charts","title":"Supported Charts","text":"<p>(see live in action)</p> <p></p>"},{"location":"architecture/data-stack/#storage-layer-postgres","title":"Storage Layer - Postgres","text":"<p>Let's start with the storage layer. We use Postgres, the currently\u00a0most used and loved database. Postgres is versatile and simple to use. It's a\u00a0relational database\u00a0that can be customized and scaled extensively.</p>"},{"location":"architecture/infrastructure/","title":"Infrastructure","text":"<p>Infrastructure is the part where we go into depth about how to run HelloDATA and its components on\u00a0Kubernetes. </p>"},{"location":"architecture/infrastructure/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes and its platform allow you to run and orchestrate container workloads. Kubernetes has become\u00a0popular\u00a0and is the\u00a0de-facto standard\u00a0for your cloud-native apps to (auto-)\u00a0scale-out\u00a0and deploy the various open-source tools fast, on any cloud, and locally. This is called cloud-agnostic, as you are not locked into any cloud vendor (Amazon, Microsoft, Google, etc.).</p> <p>Kubernetes is\u00a0infrastructure as code, specifically as YAML, allowing you to version and test your deployment quickly. All the resources in Kubernetes, including Pods, Configurations, Deployments, Volumes, etc., can be expressed in a YAML file using Kubernetes tools like HELM. Developers quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down and using any programming language running with a simple Dockerfile. Its management makes it accessible through its modularity and abstraction; also, with the use of Containers, you can monitor all your applications in one place.</p> <p>Kubernetes\u00a0Namesspaces\u00a0provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace but not across namespaces. Namespace-based scoping is applicable only for namespaced\u00a0objects (e.g. Deployments, Services, etc)\u00a0and not for cluster-wide objects\u00a0(e.g., StorageClass, Nodes, PersistentVolumes, etc).</p> <ul> <li>Namespaces provide a mechanism for isolating groups of resources within a single cluster (separation of concerns). Namespaces also lets you easily wramp up several HelloDATA instances on demand.\u00a0<ul> <li>Names of resources need to be unique within a namespace but not across namespaces.</li> </ul> </li> <li>We get central monitoring and logging solutions with\u00a0Grafana,\u00a0Prometheus, and the\u00a0ELK stack (Elasticsearch, Logstash, and Kibana). As well as the Keycloak single sign-on.</li> <li>Everything runs in a single Kubernetes Cluster but can also be deployed on-prem on any Kubernetes Cluster.</li> <li>Persistent data will run within the \"Data Domain\" and must run on a\u00a0Persistent Volume\u00a0on Kubernetes or a central Postgres service (e.g., on Azure or internal).</li> </ul> <p></p>"},{"location":"architecture/infrastructure/#module-deployment-view","title":"Module deployment view","text":"<p>Here, we have a look at the module view with an inside view of accessing the\u00a0HelloDATA Portal.</p> <p>The Portal API serves with\u00a0SpringBoot,\u00a0Wildfly\u00a0and\u00a0Angular.</p> <p></p> <p></p>"},{"location":"architecture/infrastructure/#storage-data-domain","title":"Storage (Data Domain)","text":"<p>Following up on how storage is persistent for the\u00a0Domain View\u00a0introduced in the above chapters.\u00a0</p>"},{"location":"architecture/infrastructure/#data-domain-storage-view","title":"Data-Domain Storage View","text":"<p>Storage is an important topic, as this is where the business value and the data itself are stored.</p> <p>From a Kubernetes and deployment view, everything is encapsulated inside a Namespace. As explained in the above \"Domain View\", we have different layers from one Business domain (here Business Domain) to n (multiple) Data Domains.\u00a0</p> <p>Each domain holds its data on\u00a0persistent storage, whether Postgres for relational databases, blob storage for files or file storage on persistent volumes within Kubernetes.</p> <p>GitSync is a tool we added to allow\u00a0GitOps-type deployment. As a user, you can push changes to your git repo, and GitSync will automatically deploy that into your cluster on Kubernetes.</p> <p></p>"},{"location":"architecture/infrastructure/#business-domain-storage-view","title":"Business-Domain Storage View","text":"<p>Here is another view that persistent storage within Kubernetes (K8s) can hold data across the Data Domain. If these\u00a0persistent volumes\u00a0are used to store Data Domain information, it will also require implementing a backup and restore plan for these data.</p> <p>Alternatively, blob storage on any\u00a0cloud vendor or services\u00a0such as Postgres service can be used, as these are typically managed and come with features such as backup and restore.</p> <p></p>"},{"location":"architecture/infrastructure/#k8s-jobs","title":"K8s Jobs","text":"<p>HelloDATA uses Kubernetes jobs to perform certain activities</p>"},{"location":"architecture/infrastructure/#cleanup-jobs","title":"Cleanup Jobs","text":"<p>Contents:</p> <ul> <li>Cleaning up user activity logs</li> <li>Cleaning up logfiles</li> </ul> <p></p>"},{"location":"architecture/infrastructure/#deployment-platforms","title":"Deployment Platforms","text":"<p>HelloDATA can be operated as different platforms, e.g. development, test, and/or production platforms. The deployment is based on common CICD principles. It uses GIT and flux internally to deploy its resources onto the specific Kubernetes clusters. In case of resource shortages, the underlying platform can be extended with additional resources upon request. Horizontal scaling of the infrastructure can be done within the given resources boundaries (e. g. multiple pods for Superset.)</p>"},{"location":"architecture/infrastructure/#platform-authentication-authorization","title":"Platform Authentication Authorization","text":"<p>See at\u00a0Roles and authorization concept.</p>"},{"location":"concepts/showcase/","title":"Showcase: Animal Statistics (Switzerland)","text":""},{"location":"concepts/showcase/#what-is-the-showcase","title":"What is the Showcase?","text":"<p>It's the demo cases of HD-BE, it's importing animal data from an external source and loading them with Airflow, modeled with dbt, and visualized in Superset.</p> <p>It hopefully will show you how the platform works and it comes pre-installed with the docker-compose installation.</p>"},{"location":"concepts/showcase/#how-can-i-get-started-and-explore-it","title":"How can I get started and explore it?","text":"<p>Click on the data-domain <code>showcase</code> and you can explore pre-defined dashboards with below described Airflow job and dbt models.</p>"},{"location":"concepts/showcase/#how-does-it-look","title":"How does it look?","text":"<p>Below the technical details of the showcase are described. How the airflow pipeline is collecting the data from an open API and modeling it with dbt.</p>"},{"location":"concepts/showcase/#airflow-pipeline","title":"Airflow Pipeline","text":"<ul> <li>data_download     The source files, which are in CSV format, are queried via the data_download task and stored in the file system.  </li> <li>create_tables     Based on the CSV files, tables are created in the LZN database schema of the project.  </li> <li>insert_data     After the tables have been created, in this step, the source data from the CSV file is copied into the corresponding tables in the LZN database schema.  </li> <li>dbt_run     After the preceding steps have been executed and the data foundation for the DBT framework has been established, the data processing steps in the database can be initiated using DBT scripts. (described in the DBT section)  </li> <li>dbt_docs     Upon completion of generating the tables in the database, a documentation of the tables and their dependencies is generated using DBT.  </li> <li>dbt_docs_serve     For the visualization of the generated documentation, it is provided in the form of a website.</li> </ul>"},{"location":"concepts/showcase/#dbt-data-modeling","title":"DBT: Data modeling","text":""},{"location":"concepts/showcase/#fact_breeds_long","title":"fact_breeds_long","text":"<p>The fact table fact_breeds_long describes key figures, which are used to derive the stock of registered, living animals, divided by breeds over time.</p> <p>The following tables from the [tierstatistik_lzn] database schema are selected for the calculation of the key figure:</p> <ul> <li>cats_breeds</li> <li>cattle_breeds</li> <li>dogs_breeds</li> <li>equids_breeds</li> <li>goats_breeds</li> <li>sheep_breeds</li> </ul> <p> </p>"},{"location":"concepts/showcase/#fact_cattle_beefiness_fattissue","title":"fact_cattle_beefiness_fattissue","text":"<p>The fact table fact_catle_beefiness_fattissue describes key figures, which are used to derive the number of slaughtered cows by year and month. Classification is done according to CH-TAX (Trading Class Classification CHTAX System | VIEGUT AG)</p> <p>The following tables from the [tierstatistik_lzn] database schema are selected for the calculation of the key figure:</p> <ul> <li>cattle_evolbeefiness</li> <li>cattle_evolfattissue</li> </ul> <p></p>"},{"location":"concepts/showcase/#fact_cattle_popvariations","title":"fact_cattle_popvariations","text":"<p>The fact table fact_cattle_popvariations describes key figures, which are used to derive the increase and decrease of the cattle population in the Animal Traffic Database (https://www.agate.ch/) over time (including reports from Liechtenstein). The key figures are grouped according to the following types of reports:</p> <ul> <li>Birth</li> <li>Slaughter</li> <li>Death  </li> </ul> <p>The following table from the [tierstatistik_lzn] database schema is selected for the calculation of the key figure:</p> <ul> <li>cattle_popvariations</li> </ul> <p> </p>"},{"location":"concepts/showcase/#fact_cattle_pyr_wide-fact_cattle_pyr_long","title":"fact_cattle_pyr_wide\u00a0&amp;\u00a0fact_cattle_pyr_long","text":"<p>The fact table fact_cattle_popvariations describes key figures, which are used to derive the distribution of registered living cattle by age class and gender.</p> <p>The following table from the [tierstatistik_lzn] database schema is selected for the calculation of the key figure:</p> <ul> <li>cattle_pyr</li> </ul> <p>The fact table fact_cattle_pyr_long pivots all key figures from fact_cattle_pyr_wide.</p> <p></p>"},{"location":"concepts/showcase/#superset","title":"Superset","text":""},{"location":"concepts/showcase/#database-connection","title":"Database Connection","text":"<p>The data foundation of the Superset visualizations in the form of Datasets, Dashboards, and Charts is realized through a Database Connection.  </p> <p>In this case, a database connection to a database is established, which refers to a PostgreSQL database in which the above-described DBT scripts were executed.</p>"},{"location":"concepts/showcase/#datasets","title":"Datasets","text":"<p>Datasets are used to prepare the data foundation in a suitable form, which can then be visualized in charts in an appropriate way.</p> <p>Essentially, modeled fact tables from the UDM database schema are selected and linked with dimension tables.</p> <p>This allows facts to be calculated or evaluated at different levels of professional granularity.</p> <p></p>"},{"location":"concepts/showcase/#interfaces","title":"Interfaces","text":""},{"location":"concepts/showcase/#tierstatistik","title":"Tierstatistik","text":"Source Description https://tierstatistik.identitas.ch/de/ Website of the API provider https://tierstatistik.identitas.ch/de/docs.html Documentation of the platform and description of the data basis and API tierstatistik.identitas.ch/tierstatistik.rdf API and data provided by the website"},{"location":"concepts/workspaces/","title":"Data Engineering Workspaces","text":"<p>On this page, we'll explain what workspaces in the context of HelloDATA-BE are and how to use them, and you'll create your own based on a prepared boiler-plate repo.</p> <p>Info</p> <p>Also see the step-by-step video we created that might help you further.</p>"},{"location":"concepts/workspaces/#what-is-a-workspace","title":"What is a Workspace?","text":"<p>Within the context of HelloDATA-BE, data, engineers, or technical people can\u00a0develop their dbt, airflow, or even bring their tool, all packed into a separate git-repo and run as part of HelloDATA-BE where they enjoy the benefits of persistent storage, visualization tools, user management, monitoring, etc.</p> <p><pre><code>graph TD\n    subgraph \"Business Domain (Tenant)\"\n        BD[Business Domain]\n        BD --&gt;|Services| SR1[Portal]\n        BD --&gt;|Services| SR2[Orchestration]\n        BD --&gt;|Services| SR3[Lineage]\n        BD --&gt;|Services| SR5[Database Manager]\n        BD --&gt;|Services| SR4[Monitoring &amp; Logging]\n    end\n    subgraph \"Workspaces\"\n        WS[Workspaces] --&gt;|git-repo| DE[Data Engineering]\n        WS[Workspaces] --&gt;|git-repo| ML[ML Team]\n        WS[Workspaces] --&gt;|git-repo| DA[Product Analysts]\n        WS[Workspaces] --&gt;|git-repo| NN[...]\n    end\n    subgraph \"Data Domain (1-n)\"\n        DD[Data Domain] --&gt;|Persistent Storage| PG[Postgres]\n        DD[Data Domain] --&gt;|Data Modeling| DBT[dbt]\n        DD[Data Domain] --&gt;|Visualization| SU[Superset]\n    end\n\n    BD --&gt;|Contains 1-n| DD\n    DD --&gt;|n-instances| WS\n\n    %% Colors\n    class BD business\n    class DD data\n    class WS workspace\n    class SS,PGA subsystem\n    class SR1,SR2,SR3,SR4 services\n\n    classDef business fill:#96CD70,stroke:#333,stroke-width:2px;\n    classDef data fill:#A898D8,stroke:#333,stroke-width:2px;\n    classDef workspace fill:#70AFFD,stroke:#333,stroke-width:2px;\n    %% classDef subsystem fill:#F1C40F,stroke:#333,stroke-width:2px;\n    %% classDef services fill:#E74C3C,stroke:#333,stroke-width:1px;</code></pre> A schematic overview of workspaces are embedded into HelloDATA-BE.</p> <p>A workspace can have n-instances within a data domain. What does it mean? Each team can deal with its requirements to develop and build their project independently.</p> <p>Think of an ML engineer who needs heavy tools such as Tensorflow, etc., as an analyst might build simple dbt models. In contrast, another data engineer uses a specific tool from the Modern Data Stack.</p>"},{"location":"concepts/workspaces/#when-to-use-workspaces","title":"When to use Workspaces","text":"<p>Workspaces are best used for development, implementing custom business logic, and modeling your data. But there is no limit to what you build as long as it can be run as a DAG as an Airflow data pipeline.</p> <p>Generally speaking, a workspace is used whenever someone needs to create a custom logic yet to be integrated within the HelloDATA BE Platform.</p> <p>As a second step - imagine you implemented a critical business transformation everyone needs - that code and DAG could be moved and be a default DAG within a data domain. But the development always happens within the workspace, enabling self-serve.</p> <p>Without workspaces, every request would need to go over the HelloDATA BE Project team. Data engineers need a straightforward way isolated from deployment where they can add custom code for their specific data domain pipelines.</p>"},{"location":"concepts/workspaces/#how-does-a-workspace-work","title":"How does a Workspace work?","text":"<p>When you create your workspace, it will be deployed within HelloDATA-BE and run by an Airflow DAG. The Airflow DAG is the integration into HD. You'll define things like how often it runs, what it should run, the order of it, etc.</p> <p>Below, you see an example of two different Airflow DAGs deployed from two different Workspaces (marked red arrow): </p>"},{"location":"concepts/workspaces/#how-do-i-create-my-own-workspace","title":"How do I create my own Workspace?","text":"<p>To implement your own Workspace, we created a boiler-plate repo. This repo contains a minimal set of artefacts in order to be deployed on HD.</p>"},{"location":"concepts/workspaces/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Clone boiler-plate repo.</li> <li>Add your own custom logic to the repo, update Dockerfile with relevant libraries and binaries you need.</li> <li>Create one or multiple Airflow DAGs for running within HelloDATA-BE.</li> <li>Define needed ENV-Variables and deployments needs (to be set-up by HD-Team initially once)</li> <li>Build a docker image</li> <li>Ask HD Team to deploy initially</li> </ol> <p>From now on whenever you have a change, you just build a new image and that will be deployed on HelloDATA-BE automatically. Making you and your team independent.</p>"},{"location":"concepts/workspaces/#boiler-plate-example","title":"Boiler-Plate Example","text":"<p>Below you find examples of the boiler-plate example that help you understand how to configure workspaces for your needs.</p>"},{"location":"concepts/workspaces/#boiler-plate-repo","title":"Boiler-Plate repo","text":"<p>The repo helps you to build your workspace by simply clone the whole repo and adding your changes.</p> <p>We generally have these boiler plate files: <pre><code>\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 build-and-push.sh\n\u251c\u2500\u2500 deployment\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 deployment-needs.yaml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 dags\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 airflow\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 .astro\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 config.yaml\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Dockerfile\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Makefile\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 README.md\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 airflow_settings.yaml\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 dags\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 .airflowignore\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 boiler-example.py\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 include\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 .kube\n    \u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 packages.txt\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 plugins\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 requirements.txt\n    \u2514\u2500\u2500 duckdb\n        \u2514\u2500\u2500 query_duckdb.py\n</code></pre></p>"},{"location":"concepts/workspaces/#important-files-business-logic-dag","title":"Important files: Business logic (DAG)","text":"<p>Where as <code>query_duckdb.py</code> and the <code>boiler-example.py</code> DAG are in this case are my custom code that you'd change with your own code. </p> <p>Although the Airflow DAG can be re-used as we use <code>KubernetesPodOperator</code> that works works within HD and locally (check more below). Essentially you change the name and the schedule to your needs, the image name and your good to go.</p> <p>Example of a Airflow DAG: <pre><code>from pendulum import datetime\nfrom airflow import DAG\nfrom airflow.configuration import conf\nfrom airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (\nKubernetesPodOperator,\n)\nfrom kubernetes.client import models as k8s\nimport os\ndefault_args = {\n\"owner\": \"airflow\",\n\"depend_on_past\": False,\n\"start_date\": datetime(2021, 5, 1),\n\"email_on_failure\": False,\n\"email_on_retry\": False,\n\"retries\": 1,\n}\nworkspace_name = os.getenv(\"HD_WS_BOILERPLATE_NAME\", \"ws-boilerplate\")\nnamespace = os.getenv(\"HD_NAMESPACE\", \"default\")\n# This will use .kube/config for local Astro CLI Airflow and ENV variable for k8s deployment\nif namespace == \"default\":\nconfig_file = \"include/.kube/config\"  # copy your local kube file to the include folder: `cp ~/.kube/config include/.kube/config`\nin_cluster = False\nelse:\nin_cluster = True\nconfig_file = None\nwith DAG(\ndag_id=\"run_boiler_example\",\nschedule=\"@once\",\ndefault_args=default_args,\ndescription=\"Boiler Plate for running a hello data workspace in airflow\",\ntags=[workspace_name],\n) as dag:\nKubernetesPodOperator(\nnamespace=namespace,\nimage=\"my-docker-registry.com/hellodata-ws-boilerplate:0.1.0\",\nimage_pull_secrets=[k8s.V1LocalObjectReference(\"regcred\")],\nlabels={\"pod-label-test\": \"label-name-test\"},\nname=\"airflow-running-dagster-workspace\",\ntask_id=\"run_duckdb_query\",\nin_cluster=in_cluster,  # if set to true, will look in the cluster, if false, looks for file\ncluster_context=\"docker-desktop\",  # is ignored when in_cluster is set to True\nconfig_file=config_file,\nis_delete_operator_pod=True,\nget_logs=True,\n# please add/overwrite your command here\ncmds=[\"/bin/bash\", \"-cx\"],\narguments=[\n\"python query_duckdb.py &amp;&amp; echo 'Query executed successfully'\",  # add your command here\n],\n)\n</code></pre></p>"},{"location":"concepts/workspaces/#dag-how-to-test-or-run-a-dag-locally-before-deploying","title":"DAG: How to test or run a DAG locally before deploying","text":"<p>To run locally, the easiest way is to use the Astro CLI (see link for installation). With it, we can simply <code>astro start</code> or <code>astro stop</code> to start up/down.</p> <p>For local deployment we have these requirements:</p> <ul> <li>Local Docker installed (either native or Docker-Desktop)</li> <li>make sure Kubernetes is enables</li> <li>copy you local kube-file to astro: <code>cp ~/.kube/config src/dags/airflow/include/.kube/</code></li> </ul> <p>The <code>config</code> file is used from astro to run on local Kubernetes. Se more infos on Run your Astro project in a local Airflow environment.</p>"},{"location":"concepts/workspaces/#install-requirements-dockerfile","title":"Install Requirements: <code>Dockerfile</code>","text":"<p>Below is the example how to install requirements (here <code>duckdb</code>) and copy my custom code <code>src/duckdb/query_duckdb.py</code> to the image.</p> <p>Boiler-plate example: <pre><code>FROM python:3.10-slim\nRUN mkdir -p /opt/airflow/airflow_home/dags/\n\n# Copy your airflow DAGs which will be copied into bussiness domain Airflow (These DAGs will be executed by Airflow)\nCOPY ../src/dags/airflow/dags/* /opt/airflow/airflow_home/dags/\n\nWORKDIR /usr/src/app\nRUN pip install --upgrade pip\n\n# Install DuckDB (example - please add your own dependencies here)\nRUN pip install duckdb\n\n# Copy the script into the container\nCOPY src/duckdb/query_duckdb.py ./\n\n# long-running process to keep the container running \nCMD tail -f /dev/null\n</code></pre></p>"},{"location":"concepts/workspaces/#deployment-deployment-needsyaml","title":"Deployment: <code>deployment-needs.yaml</code>","text":"<p>Below you see an an example of a deployment needs in <code>deployment-needs.yaml</code>, that defines:</p> <ul> <li>Docker image</li> <li>Volume mounts you need</li> <li>a command to run </li> <li>container behaviour</li> <li>extra ENV variables and values that HD-Team needs to provide for you</li> </ul> <p>This part is the one that will change most likely</p> <p>All of which will be eventually more automated. Also let us know or just add missing specs to the file and we'll add the functionallity on the deployment side. </p> <pre><code>spec:\ninitContainers:\ncopy-dags-to-bd:\nimage:\nrepository: my-docker-registry.com/hellodata-ws-boilerplate\npullPolicy: IfNotPresent\ntag: \"0.1.0\"\nresources: {}\nvolumeMounts:\n- name: storage-hellodata\ntype: external\npath: /storage\ncommand: [ \"/bin/sh\",\"-c\" ]\nargs: [ \"mkdir -p /storage/${datadomain}/dags/${workspace}/ &amp;&amp; rm -rf /storage/${datadomain}/dags/${workspace}/* &amp;&amp; cp -a /opt/airflow/airflow_home/dags/*.py /storage/${datadomain}/dags/${workspace}/\" ]\ncontainers:\n- name: ws-boilerplate\nimage: my-docker-registry.com/hellodata-ws-boilerplate:0.1.0\nimagePullPolicy: Always\n#needed envs for Airflow\nairflow:\nextraEnv: |\n- name: \"HD_NAMESPACE\"\nvalue: \"${namespace}\"\n- name: \"HD_WS_BOILERPLATE_NAME\"\nvalue: \"dd01-ws-boilerplate\"\n</code></pre>"},{"location":"concepts/workspaces/#conclusion","title":"Conclusion","text":"<p>I hope this has illustrated how to create your own workspace. Otherwise let us know in the discussions or create an issue/PR.</p>"},{"location":"manuals/role-authorization-concept/","title":"Roles and authorization concept","text":""},{"location":"manuals/role-authorization-concept/#platform-authentication-authorization","title":"Platform Authentication Authorization","text":"<p>Authentication and authorizations within the various logical contexts or domains of the HelloDATA system are handled as follows.\u00a0 Authentication is handled via the OAuth 2 standard. In the case of the Canton of Bern, this is done via the central KeyCloak server. Authorizations to the various elements within a subject or Data Domain are handled via authorization within the HelloDATA portal. To keep administration simple, a role concept is applied. Instead of defining the authorizations for each user, roles receive the authorizations and the users are then assigned to the roles. The roles available in the portal have fixed defined permissions.</p>"},{"location":"manuals/role-authorization-concept/#business-domain","title":"Business Domain","text":"<p>In order for a user to gain access to a Business Domain, the user must be authenticated for the Business Domain. Users without authentication who try to access a Business Domain will receive an error message. The following two logical roles are available within a Business Domain:</p> <ul> <li>HELLODATA_ADMIN</li> <li>BUSINESS_DOMAIN_ADMIN</li> </ul>"},{"location":"manuals/role-authorization-concept/#hellodata_admin","title":"HELLODATA_ADMIN","text":"<ul> <li>Can act fully in the system.</li> </ul>"},{"location":"manuals/role-authorization-concept/#business_domain_admin","title":"BUSINESS_DOMAIN_ADMIN","text":"<ul> <li>Can manage users and assign roles (except HELLODATA_ADMIN).</li> <li>Can manage dashboard metadata.</li> <li>Can manage announcements.</li> <li>Can manage the FAQ.</li> <li>Can manage the external documentation links.</li> </ul> <p>BUSINESS_DOMAIN_ADMIN is automatically DATA_DOMAIN_ADMIN in all Data Domains within the Business Domain (see Data Domain Context).</p>"},{"location":"manuals/role-authorization-concept/#data-domain","title":"Data Domain","text":"<p>A Data Domain encapsulates all data elements and tools that are of interest for a specific issue. HalloDATA supports 1 - n Data Domains within a Business Domain.</p> <p>The resources to be protected within a Data Domain are:</p> <ul> <li>Schema of the Data Domain.</li> <li>Data mart tables of the Data Domain.</li> <li>The entire DWH environment of the Data Domain.</li> <li>Data lineage documents of the DBT projects of the Data Domain.</li> <li>Dashboards, charts, datasets within the superset instance of a Data Domain.</li> <li>Airflow DAGs of the Data Domain.</li> </ul> <p>The following three logical roles are available within a Data Domain:</p> <ul> <li>DATA_DOMAIN_VIEWER \u00a0 \u00a0</li> <li>DATA_DOMAIN_EDITOR</li> <li>DATA_DOMAIN_ADMIN</li> </ul> <p>Depending on the role assigned, users are given different permissions to act in the Data Domain. A user who has not been assigned a role in a Data Domain will generally not be granted access to any resources of that Data Domain.</p>"},{"location":"manuals/role-authorization-concept/#data_domain_viewer","title":"DATA_DOMAIN_VIEWER","text":"<ul> <li>The DATA_DOMAIN_VIEWER role is granted potential read access to dashboards of a Data Domain.</li> <li>Which dashboards of the Data Domain a DATA_DOMAIN_VIEWER user is allowed to see is administered within the user management of the HelloDATA portal.</li> <li>Only assigned dashboards are visible to a DATA_DOMAIN_VIEWER.</li> <li>Only dashboards in \"Published\" status are visible to a DATA_DOMAIN_VIEWER. A DATA_DOMAIN_VIEWER can view all data lineage documents of the Data Domain.</li> <li>A DATA_DOMAIN_VIEWER can access the links to external dashboards associated with its Data Domain. It is not checked whether the user has access in the systems outside the HelloDATA system boundary.</li> </ul>"},{"location":"manuals/role-authorization-concept/#data_domain_editor","title":"DATA_DOMAIN_EDITOR","text":"<p>Same as DATA_DOMAIN_VIEWER plus:</p> <ul> <li>The DATA_DOMAIN_EDITOR role is granted read and write access to the dashboards of a Data Domain. All dashboards are visible and editable for a DATA_DOMAIN_EDITOR. All charts used in the dashboards are visible and editable for a DATA_DOMAIN_EDITOR. All data sets used in the dashboards are visible and editable for a DATA_DOMAIN_EDITOR.</li> <li>A DATA_DOMAIN_EDITOR can create new dashboards.</li> <li>A DATA_DOMAIN_EDITOR can view the data marts of the Data Domain.</li> <li>A DATA_DOMAIN_EDITOR has access to the SQL lab in the superset.</li> </ul>"},{"location":"manuals/role-authorization-concept/#data_domain_admin","title":"DATA_DOMAIN_ADMIN","text":"<p>Same as DATA_DOMAIN_EDITOR plus:</p> <p>The DATA_DOMAIN_ADMIN role can view the airflow DAGs of the Data Domain. A DATA_DOMAIN_ADMIN can view all database objects in the DWH of the Data Domain.</p>"},{"location":"manuals/role-authorization-concept/#extra-data-domain","title":"Extra Data Domain","text":"<p>Beside the standard Data Domains there are also extra Data Domains An Extra Data Domain provides additional permissions, functions and database connections such as :</p> <ul> <li>CSV uploads to the Data Domain.</li> <li>Read permissions from one Data Domain to additional other Data Domain(s).</li> <li>Database connections to Data Domains of other databases.</li> <li>Database connections via AD group permissions.</li> <li>etc.</li> </ul> <p>These additional permissions, functions or database connections are a matter of negotiation per extra Data Domain. The additional permissions, if any, are then added to the standard roles mentioned above for the extra Data Domain.</p> <p>Row Level Security settings on Superset level can be used to additionally restrict the data that is displayed in a dashboard (e.g. only data of the own domain is displayed).</p>"},{"location":"manuals/role-authorization-concept/#system-role-to-portal-role-mapping","title":"System Role to Portal Role Mapping","text":"System Role Portal Role Portal Permission Menu / Submenu / Page in Portal Info HELLODATA_ADMIN SUPERUSER ROLE_MANAGEMENT Administration / Portal Rollenverwaltung MONITORING Monitoring DEVTOOLS Dev Tools USER_MANAGEMENT Administration / Benutzerverwaltung FAQ_MANAGEMENT Administration / FAQ Verwaltung EXTERNAL_DASHBOARDS_MANAGEMENT Unter External Dashboards Kann neue Eintr\u00e4ge erstellen und verwalten bei Seite External Dashboards DOCUMENTATION_MANAGEMENT Administration / Dokumentationsmanagement ANNOUNCEMENT_MANAGEMENT Administration/ Ank\u00fcndigungen DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration BUSINESS_DOMAIN_ADMIN BUSINESS_DOMAIN_ADMIN USER_MANAGEMENT Administration / Portal Rollenverwaltung FAQ_MANAGEMENT Dev Tools EXTERNAL_DASHBOARDS_MANAGEMENT Administration / Benutzerverwaltung DOCUMENTATION_MANAGEMENT Administration / FAQ Verwaltung ANNOUNCEMENT_MANAGEMENT Unter External Dashboards DASHBOARDS Administration / Dokumentationsmanagement Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Administration/ Ank\u00fcndigungen Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration DATA_DOMAIN_ADMIN DATA_DOMAIN_ADMIN DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DWH Data Eng, / DWH Viewer Sieht im Menu Data Eng. das Submenu DWH Viewer DATA_ENG Data Eng. / Orchestration Sieht im Menu Data Eng. das Submenu Orchestration DATA_DOMAIN_EDITOR EDITOR DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_MARTS Data Marts Sieht im Menu je einen Data Mart Link f\u00fcr alle Data Domains auf die er Zugriff hat DATA_DOMAIN_VIEWER VIEWER DASHBOARDS Dashboards Sieht im Menu Liste, dann je einen Link auf alle Data Domains auf die er Zugriff hat mit deren Dashboards auf die er Zugriff hat plus Externe Dashboards DATA_LINEAGE Data Lineage Sieht im Menu je einen Lineage Link f\u00fcr alle Data Domains auf die er Zugriff hat"},{"location":"manuals/role-authorization-concept/#system-role-to-superset-role-mapping","title":"System Role to Superset Role Mapping","text":"System Role Superset Role Info No Data Domain role Public User should not get access to Superset functions so he gets a role with no permissions. DATA_DOMAIN_VIEWER BI_VIEWER plus\u00a0roles forDashboards he was granted access to i. e. the slugified dashboard names with prefix \"D_\" Example: User is \"DATA_DOMAIN_VIEWER\" in a Data Domain. We grant the user acces to the \"Hello World\" dashboard. Then user gets the role \"BI_VIEWER\" plus the role \"D_hello_world\" in Superset. DATA_DOMAIN_EDITOR BI_EDITOR Has access to all Dashboards as he is owner of the dashboards\u00a0 plus he gets SQL Lab permissions. DATA_DOMAIN_ADMIN BI_EDITOR plus\u00a0BI_ADMIN Has access to all Dashboards as he is owner of the dashboards\u00a0 plus he gets SQL Lab permissions."},{"location":"manuals/role-authorization-concept/#system-role-to-airflow-role-mapping","title":"System Role to Airflow Role Mapping","text":"System Role Airflow Role Info HELLO_DATA_ADMIN Admin User gets DATA_DOMAIN_ADMIN role for all exisitng Data Domains and thus gets his permissions by that roles.User additionally gets the Admin role. BUSINESS_DOMAIN_ADMIN User gets DATA_DOMAIN_ADMIN role for all exisitng Data Domains and thus gets his permissions by that roles. No Data Domain role Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_VIEWER Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_EDITOR Public User should not get access to Airflow functions so he gets a role with no permissions. DATA_DOMAIN_ADMIN AF_OPERATOR plus\u00a0role corresponding to his Data Domain Key with prefix \"DD_\" Example: User is \"DATA_DOMAIN_ADMIN\" in a Data Domain with the key \"data_domain_one\". Then user gets the role \"AF_OPERATOR\" plus the role \"DD_data_domain_one\" in Airflow."},{"location":"manuals/user-manual/","title":"User Manual","text":""},{"location":"manuals/user-manual/#goal","title":"Goal","text":"<p>This use manual should enable you to use the HelloDATA platform and illustrate the features of the product and how to use them.</p> <p>\u2192 More about the Platform and its architecture you can find on\u00a0Architecture &amp; Concepts.</p>"},{"location":"manuals/user-manual/#navigation","title":"Navigation","text":""},{"location":"manuals/user-manual/#portal","title":"Portal","text":"<p>The entry page of HelloDATA is the Web Portal.</p> <ol> <li>Navigation to jump to the different capabilities of HelloDATA</li> <li>Extended status information about<ol> <li>data pipelines, containers, performance and security</li> <li>documentation and subscriptions</li> </ol> </li> <li>User and profile information of logged-in user.\u00a0</li> <li>Overview of your dashboards</li> </ol> <p></p>"},{"location":"manuals/user-manual/#business-data-domain","title":"Business &amp; Data Domain","text":"<p>As explained in Domain View, a key feature is to create business domains with n-data domains. If you have access to more than one data domain, you can switch between them by clicking the <code>drop-down</code> at the top and switch between them.</p> <p></p>"},{"location":"manuals/user-manual/#dashboards","title":"Dashboards","text":"<p>The most important navigation button is the dashboard links. If you hover over it, you'll see three options to choose from.\u00a0</p> <p>You can either click the dashboard list in the hover menu (2) to see the list of dashboards with thumbnails, or directly choose your dashboard (3).</p> <p></p>"},{"location":"manuals/user-manual/#data-lineage","title":"Data-Lineage","text":"<p>To see the data lineage (dependencies of your data tables), you have the second menu option. Again, you chose the list or directly on \"data lineage\" (2).</p> <p>Button 2 will bring you to the project site, where you choose your project and load the lineage. </p> <p>Once loaded, you see all sources (1) and dbt Projects (2). On the detail page, you can see all the beautiful and helpful documentation such as:</p> <ul> <li>table name (3)</li> <li>columns and data types (4)</li> <li>which table and model this selected object depends on (5)</li> <li>the SQL code (6)<ul> <li>as a template or complied</li> </ul> </li> <li>and dependency graph (7)<ul> <li>which you can expand to full view (8) after clicking (7)</li> <li>interactive data lineage view (9)</li> </ul> </li> </ul> <p> </p>"},{"location":"manuals/user-manual/#data-marts-viewer","title":"Data Marts Viewer","text":"<p>This view let's you access the universaal data mart (udm) layer:</p> <p></p> <p>These are cleaned and modeled data mart tables. Data marts are the tables that have been joined and cleaned from the source tables. This is effectively the latest layer of HelloDATA BE, which the Dashboards are accessing. Dashboards should not access any layer before (landing zone, data storage, or data processing).</p> <p>We use CloudBeaver for this, same as the DWH Viewer later. </p>"},{"location":"manuals/user-manual/#data-engineering","title":"Data Engineering","text":""},{"location":"manuals/user-manual/#dwh-viewer","title":"DWH Viewer","text":"<p>This is essentially a database access layer where you see all your tables, and you can write SQL queries based on your access roles with a provided tool (CloudBeaver).</p>"},{"location":"manuals/user-manual/#create-new-sql-query","title":"Create new SQL Query","text":"<p>o</p>"},{"location":"manuals/user-manual/#choose-connection-and-stored-queries","title":"Choose Connection and stored queries","text":"<p>You can chose pre-defined connections and query your data warehouse. Also you can store queries that other user can see and use as well. Run your queries with (1).</p> <p></p>"},{"location":"manuals/user-manual/#settings-and-powerful-features","title":"Settings and Powerful features","text":"<p>You can set many settings, such as user status, and many more.</p> <p> Please find all setting and features in the CloudBeaver Documentation.</p>"},{"location":"manuals/user-manual/#orchestration","title":"Orchestration","text":"<p>The orchestrator is your task manager. You tell\u00a0Airflow, our orchestrator, in which order the task will run. This is usually done ahead of time, and in the portal, you can see the latest runs and their status (successful, failed, etc.).\u00a0</p> <ul> <li>You can navigate to DAGs (2) and see all the details (3) with the DAG name, owner, runs, schedules, next run and recent.</li> <li>You can also dive deeper into Datasets, Security, Admin or similar (4)</li> <li>Airflow offers lots of different visualization modes, e.g. the Graph view (6), that allows you to see each step of this task.<ul> <li>As you can see, you can choose calendar, task duration, Gantt, etc.</li> </ul> </li> </ul> <p> </p>"},{"location":"manuals/user-manual/#administration","title":"Administration","text":"<p>Here you manage the portal configurations such as user, roles, announcements, FAQs, and documentation management.</p> <p></p>"},{"location":"manuals/user-manual/#benutzerverwaltung-user-management","title":"Benutzerverwaltung / User Management","text":""},{"location":"manuals/user-manual/#adding-user","title":"Adding user","text":"<p>First type your email and hit enter. Then choose the drop down and click on it. </p> <p>Now type the Name and hit <code>Berechtigungen setzen</code> to add the user: </p> <p>You should see something like this:</p> <p></p>"},{"location":"manuals/user-manual/#changing-permissions","title":"Changing Permissions","text":"<ol> <li>Search the user you want to give or change permission</li> <li>Scroll to the right</li> <li>Click the green edit icon</li> </ol> <p>Now choose the <code>role</code> you want to give:</p> <p></p> <p>And or give access to specific data domains:</p> <p></p> <p>See more in role-authorization-concept.</p>"},{"location":"manuals/user-manual/#portal-rollenverwaltung-portal-role-management","title":"Portal Rollenverwaltung / Portal Role Management","text":"<p>In this portal role management, you can see all the roles that exist.</p> <p>Warning</p> <p>Creating new roles are not supported, despite the fact \"Rolle erstellen\" button exists. All roles are defined and hard coded.</p> <p></p>"},{"location":"manuals/user-manual/#creating-a-new-role","title":"Creating a new role","text":"<p>See how to create a new role below: </p>"},{"location":"manuals/user-manual/#ankundigung-announcement","title":"Ank\u00fcndigung / Announcement","text":"<p>You can simply create an announcement that goes to all users by <code>Ank\u00fcndigung erstellen</code>: </p> <p>Then you fill in your message. Save it.</p> <p> You'll see a success if everything went well: </p> <p>And this is how it looks to the users \u2014 It will appear until the user clicks the cross to close it. </p>"},{"location":"manuals/user-manual/#faq","title":"FAQ","text":"<p>The FAQ works the same as the announcements above. They are shown on the starting dashboard, but you can set the granularity of a data domain:</p> <p></p> <p>And this is how it looks: </p>"},{"location":"manuals/user-manual/#dokumentationsmanagement-documentation-management","title":"Dokumentationsmanagement / Documentation Management","text":"<p>Lastly, you can document the system with documentation management. Here you have one document that you can document everything in detail, and everyone can write to it. It will appear on the dashboard as well:</p> <p></p>"},{"location":"manuals/user-manual/#monitoring","title":"Monitoring","text":"<p>We provide two different ways of monitoring:\u00a0</p> <ul> <li>Status:\u00a0</li> <li>Workspaces</li> </ul> <p></p>"},{"location":"manuals/user-manual/#status","title":"Status","text":"<p>It will show you details information on instances of HelloDATA, how is the situation for the Portal, is the monitoring running, etc. </p>"},{"location":"manuals/user-manual/#data-domains","title":"Data Domains","text":"<p>In Monitoring your data domains you see each system and the link to the native application. You can easily and quickly observer permission, roles and users by different subsystems (1). Click the one you want, and you can choose different levels (2) for each, and see its permissions (3).</p> <p></p> <p></p> <p>By clicking on the blue underlined <code>DBT Docs</code>, you will be navigated to the native dbt docs. Same is true if you click on a Airflow or Superset instance.</p>"},{"location":"manuals/user-manual/#devtools","title":"DevTools","text":"<p>DevTools are additional tools HelloDATA provides out of the box to e.g. send Mail (Mailbox) or browse files (FileBrowser).</p> <p></p>"},{"location":"manuals/user-manual/#mailbox","title":"Mailbox","text":"<p>You can check in Mailbox (we use\u00a0MailHog) what emails have been sending or what accounts are updated.|</p> <p></p>"},{"location":"manuals/user-manual/#filebrowser","title":"FileBrowser","text":"<p>Here you can browse all the documentation or code from the git repos as file browser. We use\u00a0FileBrowser\u00a0here. Please use with care, as some of the folder are system relevant.</p> <p>Log in</p> <p>Make sure you have the login credentials to log in. Your administrator should be able to provide these to you.</p> <p></p>"},{"location":"manuals/user-manual/#more-know-how","title":"More: Know-How","text":"<ul> <li>More help for Superset<ul> <li>Superset Documentation</li> </ul> </li> <li>More help for dbt:<ul> <li>dbt Documentation</li> <li>dbt Developer Hub</li> </ul> </li> <li>More about Airflow<ul> <li>Airflow Documentation</li> </ul> </li> </ul> <p>Find further important references, know-how, and best practices on\u00a0HelloDATA Know-How.</p>"},{"location":"more/changelog/","title":"Changelog","text":""},{"location":"more/changelog/#2023-11-22-concepts","title":"2023-11-22 Concepts","text":"<ul> <li>Added workspaces on the concepts page.</li> <li>Added showcase main category to explain the demo that comes with HD-BE</li> </ul>"},{"location":"more/changelog/#2023-11-20-changed-corporate-design","title":"2023-11-20 Changed corporate design","text":"<ul> <li>Changed primary color to KAIO style guide: color red (#EE0F0F), and font: Roboto (was already default font)</li> </ul>"},{"location":"more/changelog/#2023-11-06-switched-architecture-over","title":"2023-11-06 Switched architecture over","text":"<ul> <li>Switched the architecture over to mkdocs</li> <li>Updated vision</li> <li>Updated user manual</li> </ul>"},{"location":"more/changelog/#2023-09-29-initial-version","title":"2023-09-29 Initial version","text":"<ul> <li>Created the template for documentation with mkdocs and the popular theme mkdocs-material.</li> </ul>"},{"location":"more/faq/","title":"FAQ","text":""},{"location":"more/glossary/","title":"Glossary","text":"<ul> <li>HD: HelloDATA</li> <li>KAIO: Amt f\u00fcr Informatik und Organisation des Kantons Bern (KAIO)</li> </ul>"},{"location":"test/examples/","title":"Examples","text":""},{"location":"test/examples/#code-annotation-examples","title":"Code Annotation Examples","text":""},{"location":"test/examples/#codeblocks","title":"Codeblocks","text":"<p>Some <code>code</code> goes here.</p>"},{"location":"test/examples/#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>Some code here\ndef myfunction()\n// some comment\n</code></pre>"},{"location":"test/examples/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start:</p> <pre><code>import tensorflow as tf\ndef whatever()\n</code></pre>"},{"location":"test/examples/#with-a-title","title":"With a title","text":"bubble_sort.py<pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#with-line-numbers","title":"With line numbers","text":"<pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"test/examples/#admonitions-call-outs","title":"Admonitions / Call-outs","text":"<p>Note</p> <p>this is a note</p> <p>Phasellus posuere in sem ut cursus</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Supported types:</p> <ul> <li>note</li> <li>abstract</li> <li>info</li> <li>tip</li> <li>success</li> <li>question</li> <li>warning</li> <li>failure</li> <li>danger</li> <li>bug</li> <li>example</li> <li>quote</li> </ul>"},{"location":"test/examples/#diagrams","title":"Diagrams","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"test/examples/#sequence-diagram","title":"Sequence diagram","text":"<p>Sequence diagrams describe a specific scenario as sequential interactions between multiple objects or actors, including the messages that are exchanged between those actors:</p> <pre><code>sequenceDiagram\n  autonumber\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre>"},{"location":"test/examples/#state-diagram","title":"State diagram","text":"<p>State diagrams are a great tool to describe the behavior of a system, decomposing it into a finite number of states, and transitions between those states: <pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre></p>"},{"location":"test/examples/#class-diagram","title":"Class diagram","text":"<p>Class diagrams are central to object oriented programing, describing the structure of a system by modelling entities as classes and relationships between them:</p> <pre><code>classDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }</code></pre>"},{"location":"test/examples/#entity-relationship-diagram","title":"Entity-relationship diagram","text":"<p>An entity-relationship diagram is composed of entity types and specifies relationships that exist between entities. It describes inter-related things in a specific domain of knowledge:</p> <pre><code>erDiagram\n  CUSTOMER ||--o{ ORDER : places\n  ORDER ||--|{ LINE-ITEM : contains\n  LINE-ITEM {\n    string name\n    int pricePerUnit\n  }\n  CUSTOMER }|..|{ DELIVERY-ADDRESS : uses</code></pre>"},{"location":"test/examples/#icons-and-emojs","title":"Icons and Emojs","text":""},{"location":"vision/roadmap/","title":"Roadmap","text":"Feature Roadmap"},{"location":"vision/vision-and-goal/","title":"Our Vision and Goal","text":"<p>The Open-Source Enterprise Data Platform in a Single Portal</p> <p>HelloDATA BE is an enterprise data platform built on top of open source. We use state-of-the-art tools such as dbt for data modeling with SQL and Airflow to run and orchestrate tasks and use Superset to visualize the BI dashboards. The underlying database is Postgres.</p>"},{"location":"vision/vision-and-goal/#vision","title":"Vision","text":"<p>In a fast-moving data engineering world, where every device and entity becomes a data generator, the need for agile, robust, and transparent data platforms is more crucial. HelloDATA BE is not just any data platform; it's the bridge between open-source innovation and enterprise solutions' demanding reliability. </p> <p>HelloDATA BE handpicked the best tools like dbt, Airflow, Superset, and Postgres and integrated them into a seamless, enterprise-ready data solution. Empowering businesses with the agility of open-source and the dependability of a tested, unified platform. </p>"},{"location":"vision/vision-and-goal/#the-goal-of-hellodata-be","title":"The Goal of HelloDATA BE","text":"<p>Our goal at HelloDATA BE is clear: to democratize the power of data for enterprises. </p> <p>As digital transformation and data expand, the challenges with various SaaS solutions, vendor lock-ins, and fragmented data sources become apparent. </p> <p>HelloDATA BE trying to provide an answer to these challenges. We aim to merge the world's best open-source tools, refining them for enterprise standards ensuring that every organization, irrespective of size or niche, has access to top-tier data solutions. By fostering a community-driven approach through our open-source commitment, we envision a data future that's inclusive, robust, and open to innovation.</p>"}]}